{
  "symbol": "G10L",
  "title": "SPEECH ANALYSIS TECHNIQUES OR SPEECH SYNTHESIS SPEECH RECOGNITION SPEECH OR VOICE PROCESSING TECHNIQUES SPEECH OR AUDIO CODING OR DECODING",
  "children": [
    {
      "symbol": "G10L13/00",
      "title": "",
      "children": [
        {
          "symbol": "G10L13/00",
          "title": "Speech synthesis Text to speech systems",
          "children": [
            {
              "symbol": "G10L13/02",
              "title": "Methods for producing synthetic speech Speech synthesisers",
              "children": [
                {
                  "symbol": "G10L2013/021",
                  "title": "Overlap-add techniques",
                  "children": [],
                  "expanded_definition": "G10L2013/021 covers techniques that use an overlap-add approach for synthesizing speech. In overlap-add techniques, small segments of speech are generated and combined by overlapping and adding them together to produce the synthesized speech output."
                },
                {
                  "symbol": "G10L13/027",
                  "title": "Concept to speech synthesisers Generation of natural phrases from machine-based concepts",
                  "children": [],
                  "expanded_definition": "G10L13/027 covers techniques for generating natural language phrases or sentences from machine-based conceptual representations or semantic structures, as opposed to directly converting text to speech. The focus is on converting abstract concepts or meanings into corresponding natural utterances using speech synthesis."
                },
                {
                  "symbol": "G10L13/033",
                  "title": "Voice editing, e.g. manipulating the voice of the synthesiser",
                  "children": [
                    {
                      "symbol": "G10L13/0335",
                      "title": "Pitch control",
                      "children": [],
                      "expanded_definition": "G10L13/0335: 'Pitch control' covers techniques specifically focused on controlling or manipulating the pitch of a synthesized or edited voice signal within voice editing systems. This may include pitch shifting, pitch quantization, pitch correction, etc."
                    }
                  ],
                  "expanded_definition": "G10L13/033 covers techniques and methods specifically related to voice editing or manipulating the voice output of a speech synthesizer. This includes modifying characteristics like pitch, tone, timbre, or other vocal qualities of the synthesized speech output."
                },
                {
                  "symbol": "G10L13/04",
                  "title": "Details of speech synthesis systems, e.g. synthesiser structure or memory management",
                  "children": [
                    {
                      "symbol": "G10L13/047",
                      "title": "Architecture of speech synthesisers",
                      "children": [],
                      "expanded_definition": "G10L13/047: This category covers the specific architectural designs and structural implementations of speech synthesizers, which are systems that artificially produce human-like speech output. It encompasses various architectural frameworks, configurations, and organizational structures employed in the construction and operation of speech synthesis systems."
                    }
                  ],
                  "expanded_definition": "G10L13/04 covers specific details and implementations related to the structure and design of speech synthesis systems, as well as memory management approaches employed in these systems. This includes aspects such as the architectural components, modules, and their interconnections within a speech synthesizer, as well as techniques for efficient memory allocation, utilization, and management to support the speech synthesis process."
                }
              ],
              "expanded_definition": "G10L13/02 covers methods and systems specifically for producing synthetic speech using speech synthesizers. This category is focused on the techniques and algorithms used in text-to-speech (TTS) systems to generate artificial speech output from input text."
            },
            {
              "symbol": "G10L13/06",
              "title": "Elementary speech units used in speech synthesisers Concatenation rules",
              "children": [
                {
                  "symbol": "G10L13/07",
                  "title": "Concatenation rules",
                  "children": [],
                  "expanded_definition": "G10L13/07: Concatenation rules\n\nThis subgroup covers specific rules and methods for concatenating elementary speech units (e.g. phones, diphones, syllables) when synthesizing speech using concatenative synthesis techniques. It deals with the processes and algorithms for smoothly joining and combining the individual speech units into natural-sounding synthetic speech output."
                }
              ],
              "expanded_definition": "G10L13/06 covers rules and methods for concatenating elementary speech units (e.g. phones, diphones, syllables) when synthesizing speech in text-to-speech systems."
            },
            {
              "symbol": "G10L13/08",
              "title": "Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination",
              "children": [
                {
                  "symbol": "G10L2013/083",
                  "title": "Special characters, e.g. punctuation marks",
                  "children": [],
                  "expanded_definition": "G10L2015/083 covers handling of special characters like punctuation marks during text analysis or generation of speech synthesis parameters from text input."
                },
                {
                  "symbol": "G10L13/086",
                  "title": "Detection of language",
                  "children": [],
                  "expanded_definition": "G10L13/086 covers techniques for detecting the language of input text or speech data."
                },
                {
                  "symbol": "G10L13/10",
                  "title": "Prosody rules derived from text Stress or intonation",
                  "children": [
                    {
                      "symbol": "G10L2013/105",
                      "title": "Duration",
                      "children": [],
                      "expanded_definition": "G10L2013/105: This category covers techniques and methods specifically related to determining or modeling the duration or timing aspects of speech prosody, such as phone or syllable durations, when deriving prosody rules from text for speech synthesis or recognition."
                    }
                  ],
                  "expanded_definition": "G10L13/10 covers techniques for deriving prosody rules, specifically those related to determining stress patterns or intonation contours, from textual input for speech synthesis purposes."
                }
              ],
              "expanded_definition": "G10L13/08 covers techniques specifically related to analyzing text and generating parameters for speech synthesis from that text. This includes grapheme-to-phoneme translation (converting written text into a phonetic representation), as well as generating prosodic features like stress, intonation patterns, and other elements that contribute to more natural-sounding synthetic speech output."
            }
          ],
          "expanded_definition": "G10L13/00 covers systems and methods specifically for converting text data into synthesized speech output, commonly known as text-to-speech (TTS) systems. This category is focused on the techniques and algorithms used to generate artificial speech waveforms or speech parameters from input text."
        },
        {
          "symbol": "G10L15/00",
          "title": "Speech recognition",
          "children": [
            {
              "symbol": "G10L15/005",
              "title": "Language recognition",
              "children": [],
              "expanded_definition": "G10L15/005 covers techniques specifically for recognizing or identifying the language being spoken in an utterance or speech signal. This is distinct from recognizing the words or content spoken, but rather determining which human language (e.g. English, Spanish, Mandarin, etc.) the speech belongs to."
            },
            {
              "symbol": "G10L15/01",
              "title": "Assessment or evaluation of speech recognition systems",
              "children": [],
              "expanded_definition": "G10L15/01: This category covers methods and systems specifically related to assessing or evaluating the performance of speech recognition systems. It includes techniques for measuring accuracy, robustness, or other metrics to evaluate the capabilities and limitations of speech recognition models or engines."
            },
            {
              "symbol": "G10L15/02",
              "title": "Feature extraction for speech recognition Selection of recognition unit",
              "children": [
                {
                  "symbol": "G10L2015/022",
                  "title": "Demisyllables, biphones or triphones being the recognition units",
                  "children": [],
                  "expanded_definition": "The CPC category G10L2015/022 covers speech recognition systems that use demisyllables, biphones, or triphones as the fundamental recognition units or modeling units for acoustic feature extraction and speech decoding.\n\nSpecifically:\n\n- Demisyllables refer to sub-syllabic units that are half of a syllable, such as the onset or rime.\n- Biphones are units spanning the transition between two consecutive phones (basic speech sounds).\n- Triphones are units representing the context of a phone with its preceding and following phones.\n\nUsing these sub-word units as recognition units can provide more detailed acoustic modeling compared to whole words or syllables, while still being more tractable than modeling at the phone level."
                },
                {
                  "symbol": "G10L2015/025",
                  "title": "Phonemes, fenemes or fenones being the recognition units",
                  "children": [],
                  "expanded_definition": "G10L2015/025 covers speech recognition systems where the recognition units are phonemes, fenemes, or fenones. Phonemes are the basic units of sound that distinguish one word from another in a language. Fenemes and fenones are related concepts, with fenemes referring to abstract representations of speech sounds, and fenones being derived acoustic properties used to characterize fenemes."
                },
                {
                  "symbol": "G10L2015/027",
                  "title": "Syllables being the recognition units",
                  "children": [],
                  "expanded_definition": "G10L2015/027 covers speech recognition systems where syllables are used as the fundamental recognition units for feature extraction and analysis, rather than phonemes, words, or other units."
                }
              ],
              "expanded_definition": "G10L15/02 covers methods and techniques specifically related to the selection or determination of the recognition unit used for speech recognition systems. This refers to the fundamental units (such as phonemes, syllables, words, etc.) that the speech recognition system is designed to recognize and process. Methods in this category deal with algorithms and approaches for defining and identifying the appropriate recognition units from the input speech signal for the particular speech recognition task."
            },
            {
              "symbol": "G10L15/04",
              "title": "Segmentation Word boundary detection",
              "children": [
                {
                  "symbol": "G10L15/05",
                  "title": "Word boundary detection",
                  "children": [],
                  "expanded_definition": "G10L15/05: This category covers techniques specifically for detecting word boundaries within an audio or text stream. It focuses on methods to identify the start and end points of individual words, separating them from adjacent words or non-word segments."
                }
              ],
              "expanded_definition": "G10L15/04 covers techniques specifically for detecting word boundaries within spoken utterances for the purpose of speech recognition. This involves identifying the start and end points of individual words to properly segment the speech signal into word units, which aids in recognizing the word sequences."
            },
            {
              "symbol": "G10L15/06",
              "title": "Creation of reference templates Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice",
              "children": [
                {
                  "symbol": "G10L15/063",
                  "title": "Training",
                  "children": [
                    {
                      "symbol": "G10L2015/0631",
                      "title": "Creating reference templates; Clustering",
                      "children": [
                        {
                          "symbol": "G10L2015/0633",
                          "title": "using lexical or orthographic knowledge sources",
                          "children": [],
                          "expanded_definition": "G10L2015/0633 covers speech recognition techniques that utilize lexical or orthographic knowledge sources. Lexical knowledge refers to information about words, such as their spellings, pronunciations, and meanings. Orthographic knowledge relates to the conventional spelling system of a language. This category covers approaches that leverage such knowledge sources to aid in creating reference templates or performing clustering for speech recognition purposes."
                        }
                      ],
                      "expanded_definition": "G10L2015/0631: 'Creating reference templates; Clustering' covers techniques specifically related to generating reference templates or models through clustering methods during the training phase of speech recognition systems. This includes methods for grouping or clustering training data samples to create representative templates or models that can be used as references for recognizing speech input."
                    },
                    {
                      "symbol": "G10L2015/0635",
                      "title": "updating or merging of old and new templates; Mean values; Weighting",
                      "children": [
                        {
                          "symbol": "G10L2015/0636",
                          "title": "Threshold criteria for the updating",
                          "children": [],
                          "expanded_definition": "G10L2015/0636 covers techniques that use specific threshold criteria to determine when to update acoustic or language models in speech recognition systems. This category is focused on the threshold conditions that trigger model updating, rather than the general process of updating models or merging old and new data."
                        }
                      ],
                      "expanded_definition": "G10L2015/0635 covers techniques for updating or merging old and new templates (e.g., speech models or acoustic models) during the training process. This includes calculating and using mean values or applying weighting schemes when combining or merging different templates or models."
                    },
                    {
                      "symbol": "G10L2015/0638",
                      "title": "Interactive procedures",
                      "children": [],
                      "expanded_definition": "G10L2015/0638 covers interactive procedures for training speech recognition systems. It specifically relates to methods or techniques where the training process involves some form of interaction or feedback, such as active learning, user corrections, or iterative refinement of the models based on user interactions."
                    }
                  ],
                  "expanded_definition": "G10L15/063 covers methods and techniques specifically related to the training process of speech recognition systems, such as algorithms, data selection, and model optimization used during the training phase to adapt the speech recognition models to different characteristics or environments."
                },
                {
                  "symbol": "G10L15/065",
                  "title": "Adaptation",
                  "children": [
                    {
                      "symbol": "G10L15/07",
                      "title": "to the speaker",
                      "children": [
                        {
                          "symbol": "G10L15/075",
                          "title": "supervised, i.e. under machine guidance",
                          "children": [],
                          "expanded_definition": "G10L15/075: This covers speech recognition systems that are supervised or operate under machine guidance, specifically for the purpose of recognizing speech from the speaker. The machine guidance involves training or adapting the speech recognition models using supervised learning techniques, where the system is provided with labeled data consisting of speech samples and their corresponding transcriptions."
                        }
                      ],
                      "expanded_definition": "G10L15/07 - 'to the speaker'\n\nThis subgroup covers speech recognition adaptation techniques specifically tailored to individual speakers. It involves modifying or adjusting the speech recognition system's models or parameters based on characteristics of a particular speaker's voice or speaking patterns to improve recognition accuracy for that speaker."
                    }
                  ],
                  "expanded_definition": "G10L15/065 'Adaptation' covers techniques for adapting or modifying speech recognition systems to account for specific characteristics of a speaker's voice. This can involve adjusting the acoustic models or other components of the speech recognition system to better match the particular speech patterns, accent, or other vocal traits of an individual speaker or group of speakers."
                }
              ],
              "expanded_definition": "G10L15/06 covers the creation of reference templates and the training process specifically for speech recognition systems, such as adapting the system to the characteristics of the speaker's voice.\n\nThis category is focused on the process of preparing speech recognition models by creating reference templates from training data, as well as techniques for training and fine-tuning the speech recognition system itself, particularly in relation to speaker adaptation and accounting for individual speaker traits."
            },
            {
              "symbol": "G10L15/08",
              "title": "Speech classification or search",
              "children": [
                {
                  "symbol": "G10L2015/081",
                  "title": "Search algorithms, e.g. Baum-Welch or Viterbi",
                  "children": [],
                  "expanded_definition": "G10L2015/081 covers search algorithms specifically designed for speech recognition tasks, such as the Baum-Welch and Viterbi algorithms. These algorithms are used to find the most likely sequence of words or phonemes given an input audio signal, which is a key step in speech recognition systems."
                },
                {
                  "symbol": "G10L15/083",
                  "title": "Recognition networks",
                  "children": [],
                  "expanded_definition": "G10L15/083 - Recognition networks:\n\nThis category covers the use of artificial neural networks specifically designed or adapted for speech recognition tasks. It includes architectures, training methods, and applications of neural networks tailored for recognizing spoken words, phrases, or commands from audio input."
                },
                {
                  "symbol": "G10L2015/085",
                  "title": "Methods for reducing search complexity, pruning",
                  "children": [],
                  "expanded_definition": "G10L2015/085: 'Methods for reducing search complexity, pruning' covers techniques used to simplify or reduce the computational complexity involved in speech classification or search tasks. These methods aim to improve efficiency by pruning or eliminating unlikely candidates or paths during the search process, thereby reducing the search space and computational requirements."
                },
                {
                  "symbol": "G10L2015/086",
                  "title": "Recognition of spelled words",
                  "children": [],
                  "expanded_definition": "G10L2015/086: Recognition of spelled words\n\nThis category covers techniques and methods specifically focused on recognizing and detecting words that are spelled out loud, letter by letter. It deals with the problem of automatically converting a sequence of spelled letters into the intended word or phrase."
                },
                {
                  "symbol": "G10L2015/088",
                  "title": "Word spotting",
                  "children": [],
                  "expanded_definition": "G10L2015/088 - 'Word spotting' refers to techniques for detecting and locating specific spoken words or word sequences within an audio signal or speech data.\n\nIt covers methods for identifying and retrieving individual words or phrases of interest from continuous speech, as opposed to recognizing the entire speech content. The focus is on spotting predefined keywords or vocabulary items within the speech input."
                },
                {
                  "symbol": "G10L15/10",
                  "title": "using distance or distortion measures between unknown speech and reference templates",
                  "children": [],
                  "expanded_definition": "G10L15/10 covers speech classification or search techniques that use distance or distortion measures between the unknown speech input and reference speech templates. This involves calculating a similarity score or distance metric between the input speech and pre-stored reference speech templates or models, and using that score/distance to classify or match the unknown speech input."
                },
                {
                  "symbol": "G10L15/12",
                  "title": "using dynamic programming techniques, e.g. dynamic time warping [DTW]",
                  "children": [],
                  "expanded_definition": "G10L15/12 covers speech recognition techniques that specifically use dynamic programming methods, such as dynamic time warping (DTW), for aligning and matching speech patterns. These techniques are used to handle variations in speech signals like speaking rate differences."
                },
                {
                  "symbol": "G10L15/14",
                  "title": "using statistical models, e.g. Hidden Markov Models [HMMs]",
                  "children": [
                    {
                      "symbol": "G10L15/142",
                      "title": "Hidden Markov Models [HMMs]",
                      "children": [
                        {
                          "symbol": "G10L15/144",
                          "title": "Training of HMMs",
                          "children": [
                            {
                              "symbol": "G10L15/146",
                              "title": "with insufficient amount of training data, e.g. state sharing, tying, deleted interpolation",
                              "children": [],
                              "expanded_definition": "G10L15/146 covers techniques specifically for training Hidden Markov Models (HMMs) when there is an insufficient amount of training data available. This includes methods like state sharing/tying (tying states across different models), deleted interpolation (smoothing by interpolating with a deleted model), and other approaches to compensate for data scarcity during HMM training."
                            }
                          ],
                          "expanded_definition": "G10L15/144 covers techniques specifically related to the training of Hidden Markov Models (HMMs) for speech recognition or other applications involving HMMs. This includes methods for estimating the parameters of HMMs from training data, algorithms for optimizing the model parameters during the training process, and any other techniques focused on the training phase for HMM-based systems."
                        },
                        {
                          "symbol": "G10L15/148",
                          "title": "Duration modelling in HMMs, e.g. semi HMM, segmental models or transition probabilities",
                          "children": [],
                          "expanded_definition": "G10L15/148 covers techniques specific to modeling the duration or temporal segmentation of speech units within Hidden Markov Models (HMMs). This includes semi-Hidden Markov Models, segmental models, and approaches that explicitly model transition probabilities between states to account for varying durations."
                        }
                      ],
                      "expanded_definition": "G10L15/142 covers speech recognition systems that specifically use Hidden Markov Models (HMMs) as the statistical model for recognizing speech. HMMs are a type of temporal pattern recognition model that assumes an underlying latent (hidden) process generates observable data sequences. In speech recognition, HMMs model speech spectra as a Markov process and aim to find the most likely sequence of speech units (e.g. phonemes, words) that could have produced the observed speech signal."
                    }
                  ],
                  "expanded_definition": "G10L15/14 covers speech recognition methods that use statistical models, particularly Hidden Markov Models (HMMs), for classifying or searching speech data. This includes techniques that model speech using HMMs and other probabilistic models to recognize spoken words, phrases, or acoustic units from speech signals."
                },
                {
                  "symbol": "G10L15/16",
                  "title": "using artificial neural networks",
                  "children": [],
                  "expanded_definition": "G10L15/16 covers speech classification or speech search techniques that specifically use artificial neural networks.\n\nThis category is focused on the use of artificial neural network models and architectures for tasks like speech recognition, speech classification, spoken keyword detection, etc. It encompasses neural network approaches such as deep neural networks, convolutional neural networks, recurrent neural networks, and other architectures tailored for speech processing."
                },
                {
                  "symbol": "G10L15/18",
                  "title": "using natural language modelling",
                  "children": [
                    {
                      "symbol": "G10L15/1807",
                      "title": "using prosody or stress",
                      "children": [],
                      "expanded_definition": "G10L15/1807 covers speech recognition techniques that specifically utilize prosodic features or stress patterns in the speech signal for natural language modeling. This includes methods that leverage intonation, rhythm, emphasis, and other prosodic cues to aid in understanding and modeling the linguistic content and meaning conveyed through spoken language."
                    },
                    {
                      "symbol": "G10L15/1815",
                      "title": "Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning",
                      "children": [],
                      "expanded_definition": "G10L15/1815 covers methods for using semantic context or word meaning to disambiguate or improve speech recognition hypotheses within natural language modeling for speech recognition systems.\n\nIt focuses specifically on techniques that leverage the semantic meaning of words and their context to resolve ambiguities or refine the output hypotheses generated by the speech recognition engine."
                    },
                    {
                      "symbol": "G10L15/1822",
                      "title": "Parsing for meaning understanding",
                      "children": [],
                      "expanded_definition": "G10L15/1822 - 'Parsing for meaning understanding' covers techniques and methods for analyzing and interpreting the syntactic and semantic structure of natural language input with the goal of extracting the intended meaning. This includes approaches like semantic parsing, compositional semantics, and pragmatic reasoning to map linguistic elements to their denotations or interpretations within a meaning representation framework."
                    },
                    {
                      "symbol": "G10L15/183",
                      "title": "using context dependencies, e.g. language models",
                      "children": [
                        {
                          "symbol": "G10L15/187",
                          "title": "Phonemic context, e.g. pronunciation rules, phonotactical constraints or phoneme n-grams",
                          "children": [],
                          "expanded_definition": "G10L15/187 covers techniques and models that specifically account for the phonemic context in speech recognition systems. This includes:\n\n- Pronunciation rules that model how phonemes are realized in different phonetic contexts.\n- Phonotactical constraints that restrict allowable phoneme sequences based on the language's phonology.\n- Phoneme n-gram models that capture statistical patterns of phoneme co-occurrences within words or across word boundaries.\n\nThe key focus is on modeling contextual dependencies between phonemes, rather than just treating them as independent units, to better capture pronunciation variations and improve speech recognition accuracy."
                        },
                        {
                          "symbol": "G10L15/19",
                          "title": "Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules",
                          "children": [
                            {
                              "symbol": "G10L15/193",
                              "title": "Formal grammars, e.g. finite state automata, context free grammars or word networks",
                              "children": [],
                              "expanded_definition": "G10L15/193 covers formal grammars used for speech recognition, such as finite state automata, context-free grammars, and word networks."
                            },
                            {
                              "symbol": "G10L15/197",
                              "title": "Probabilistic grammars, e.g. word n-grams",
                              "children": [],
                              "expanded_definition": "G10L15/197 covers probabilistic grammars used for speech recognition, such as word n-gram language models. These models capture statistical patterns in word sequences to help disambiguate recognition hypotheses and improve accuracy."
                            }
                          ],
                          "expanded_definition": "G10L15/19 covers techniques for speech recognition that use grammatical context to disambiguate or select among different recognition hypotheses. This involves applying rules or models based on valid word sequences and grammar to resolve ambiguities in the recognized text."
                        }
                      ],
                      "expanded_definition": "G10L15/183 covers speech recognition systems that use context dependencies or language models. This refers to techniques that leverage statistical models of how words are likely to co-occur in a given language or context to aid in recognizing spoken words and phrases.\n\nFor example, if the current word hypothesis is \"the\", a language model can suggest words like \"cat\", \"dog\", or \"house\" are more probable next words than \"turnip\" based on frequently observed patterns in text data for that language. Context dependencies capture how the probabilities of word sequences depend on surrounding words."
                    }
                  ],
                  "expanded_definition": "G10L15/18 covers methods and systems for speech recognition or understanding that specifically use natural language modeling techniques. Natural language modeling refers to building statistical models of how words are used in natural human languages, including things like grammar rules, word relationships, and probabilities of word sequences. This allows speech recognition systems to better interpret the intended meaning and natural flow of spoken utterances, beyond just recognizing individual words."
                }
              ],
              "expanded_definition": "G10L15/08 covers techniques specifically related to classifying or searching speech data. This includes methods for categorizing speech segments based on attributes like language, speaker characteristics, topic, etc. It also covers systems for searching and retrieving speech data from databases or archives based on spoken queries or other speech-based input."
            },
            {
              "symbol": "G10L15/20",
              "title": "Speech recognition techniques specially adapted for robustness in adverse environments, e.g. in noise, of stress induced speech",
              "children": [],
              "expanded_definition": "G10L15/20 covers speech recognition techniques specifically designed to handle adverse environments or conditions that can degrade speech recognition performance. This includes techniques to make speech recognition more robust against external noise or interference, as well as techniques to handle speech variations caused by factors like stress or emotion in the speaker's voice."
            },
            {
              "symbol": "G10L15/22",
              "title": "Procedures used during a speech recognition process, e.g. man-machine dialogue",
              "children": [
                {
                  "symbol": "G10L2015/221",
                  "title": "Announcement of recognition results",
                  "children": [],
                  "expanded_definition": "G10L2015/221 - This category covers the specific aspect of announcing or outputting the results of the speech recognition process, such as displaying the recognized text, playing back synthesized audio of the recognized speech, or providing some other form of feedback to the user regarding what speech was successfully recognized."
                },
                {
                  "symbol": "G10L15/222",
                  "title": "Barge in, i.e. overridable guidance for interrupting prompts",
                  "children": [],
                  "expanded_definition": "G10L15/222 covers techniques and procedures for allowing the user to interrupt or \"barge in\" on prompts or guidance provided by a speech recognition system during a man-machine dialogue. This enables the user to override the system's prompts and take control of the interaction."
                },
                {
                  "symbol": "G10L2015/223",
                  "title": "Execution procedure of a spoken command",
                  "children": [],
                  "expanded_definition": "G10L2015/223 covers techniques and methods specifically related to executing or carrying out spoken commands recognized by a speech recognition system. This covers the procedures and steps involved in taking action or performing a task in response to a recognized voice command from a user."
                },
                {
                  "symbol": "G10L2015/225",
                  "title": "Feedback of the input speech",
                  "children": [],
                  "expanded_definition": "G10L2015/225: Covers techniques and methods related to providing feedback or analysis on the speech input itself during a speech recognition process, such as detecting and indicating pronunciation errors, providing real-time visual feedback on the recognized speech, or allowing the user to verify and correct the recognized speech input."
                },
                {
                  "symbol": "G10L2015/226",
                  "title": "using non-speech characteristics",
                  "children": [
                    {
                      "symbol": "G10L2015/227",
                      "title": "of the speaker;  Human-factor methodology",
                      "children": [],
                      "expanded_definition": "G10L2015/227 covers speech recognition techniques that utilize non-speech characteristics related specifically to the speaker's human factors or human-centric methodology. This may include considering aspects such as the speaker's physiology, psychology, or behavioral patterns that can aid in speaker recognition or adapting the system to individual users."
                    },
                    {
                      "symbol": "G10L2015/228",
                      "title": "of application context",
                      "children": [],
                      "expanded_definition": "G10L2015/228 covers speech recognition techniques that utilize the application context or scenario in which the speech recognition is being performed. This could involve adapting the speech recognition models, parameters, or processing based on the specific type of application, task, environment, or domain in which the speech input is being recognized. The application context provides additional information beyond just the speech signal itself to improve recognition accuracy and tailor the system's behavior."
                    }
                  ],
                  "expanded_definition": "G10L2015/226 covers speech recognition procedures that utilize non-speech characteristics or information present in the input signal or data, in addition to the speech content itself. This could include utilizing background noise, speaker traits like age or gender, acoustic environment characteristics, or other non-speech elements to aid in speech recognition."
                }
              ],
              "expanded_definition": "G10L15/22 covers specific procedures or techniques used during the speech recognition process itself, such as:\n\n- Dialogue management for man-machine interaction involving speech input\n- Handling interruptions, corrections or clarifications during speech recognition\n- Error recovery, confirmation or verification techniques applied during the recognition process\n- Adaptation or personalization of speech models/parameters based on speaker interaction\n- Techniques for integrating speech with other modalities (e.g. lip movement, gestures) during recognition\n\nThe key focus is on methods that manage and facilitate the real-time speech recognition process when interacting with a human user through dialogue or conversation scenarios."
            },
            {
              "symbol": "G10L15/24",
              "title": "Speech recognition using non-acoustical features",
              "children": [
                {
                  "symbol": "G10L15/25",
                  "title": "using position of the lips, movement of the lips or face analysis",
                  "children": [],
                  "expanded_definition": "G10L15/25 covers speech recognition techniques that use visual features related to the position, movement, or analysis of the lips or face. These are non-acoustic features derived from visually tracking the speaker's lip movements or facial expressions during speech production."
                }
              ],
              "expanded_definition": "G10L15/24 covers speech recognition techniques that utilize non-acoustic features in addition to or instead of acoustic speech signals. These non-acoustic features could include visual cues from lip movements, facial expressions, or gestures captured by video or other sensors. The key aspect is the incorporation of information beyond just the audio speech signal to aid in speech recognition."
            },
            {
              "symbol": "G10L15/26",
              "title": "Speech to text systems",
              "children": [],
              "expanded_definition": "G10L15/26: 'Speech to text systems' covers systems and methods specifically focused on converting spoken utterances or audio signals into textual form or transcriptions. This includes techniques for acoustic and language modeling tailored towards speech transcription, methods for handling disfluencies, accents, and speaker variations in transcribed text, as well as approaches for punctuation insertion, capitalization, and other text formatting aspects when transcribing speech."
            },
            {
              "symbol": "G10L15/28",
              "title": "Constructional details of speech recognition systems",
              "children": [
                {
                  "symbol": "G10L15/285",
                  "title": "Memory allocation or algorithm optimisation to reduce hardware requirements",
                  "children": [],
                  "expanded_definition": "G10L15/285 covers techniques related to optimizing speech recognition systems to reduce hardware requirements, such as memory allocation strategies or algorithmic optimizations aimed at minimizing the computational resources needed for speech recognition tasks."
                },
                {
                  "symbol": "G10L15/30",
                  "title": "Distributed recognition, e.g. in client-server systems, for mobile phones or network applications",
                  "children": [],
                  "expanded_definition": "G10L15/30 covers speech recognition systems and techniques specifically designed for distributed computing environments, such as client-server systems, mobile phone applications, or network-based applications. It relates to the constructional details and implementation aspects of speech recognition systems that involve the distribution of processing tasks or data across multiple devices or nodes in a network."
                },
                {
                  "symbol": "G10L15/32",
                  "title": "Multiple recognisers used in sequence or in parallel Score combination systems therefor, e.g. voting systems",
                  "children": [],
                  "expanded_definition": "G10L15/32 covers speech recognition systems that employ multiple recognizers or recognition models in sequence or in parallel, along with systems for combining the scores or outputs from those multiple recognizers. This includes voting systems where the outputs of multiple recognizers are combined through voting or other score combination techniques."
                },
                {
                  "symbol": "G10L15/34",
                  "title": "Adaptation of a single recogniser for parallel processing, e.g. by use of multiple processors or cloud computing",
                  "children": [],
                  "expanded_definition": "G10L15/34 covers techniques and methods for adapting or modifying a single speech recognition system or recognizer to enable parallel processing, such as by utilizing multiple processors, distributed computing resources, or cloud computing infrastructure. This subgroup specifically deals with the architectural or implementation details required to parallelize the speech recognition process across multiple computational units or environments."
                }
              ],
              "expanded_definition": "G10L15/28: 'Constructional details of speech recognition systems' covers the physical and structural aspects of speech recognition systems, including hardware components, circuit designs, system architectures, and other implementation-specific details related to constructing and implementing speech recognition systems."
            }
          ],
          "expanded_definition": "G10L15/00 covers techniques and systems specifically related to speech recognition, which is the process of automatically converting spoken words or utterances into text or other computer-recognizable forms."
        },
        {
          "symbol": "G10L17/00",
          "title": "Speaker identification or verification techniques",
          "children": [
            {
              "symbol": "G10L17/02",
              "title": "Preprocessing operations, e.g. segment selection Pattern representation or modelling, e.g. based on linear discriminant analysis [LDA] or principal components Feature selection or extraction",
              "children": [],
              "expanded_definition": "CPC category G10L17/02 covers techniques specifically related to the preprocessing, pattern representation/modeling, and feature selection/extraction stages within speaker identification or verification systems. This includes operations such as segment selection for preprocessing the input audio signal, methods for representing or modeling speaker patterns (e.g., using linear discriminant analysis or principal component analysis), and algorithms for selecting or extracting relevant features from the audio data to be used for speaker identification/verification."
            },
            {
              "symbol": "G10L17/04",
              "title": "Training, enrolment or model building",
              "children": [],
              "expanded_definition": "G10L17/04 specifically covers techniques for training, enrolling, or building models to be used in speaker identification or verification systems. This includes methods for collecting and preparing training data, algorithms for training speaker recognition models on that data, and processes for creating or updating speaker voiceprint models to be used for identification or verification purposes."
            },
            {
              "symbol": "G10L17/06",
              "title": "Decision making techniques Pattern matching strategies",
              "children": [
                {
                  "symbol": "G10L17/08",
                  "title": "Use of distortion metrics or a particular distance between probe pattern and reference templates",
                  "children": [],
                  "expanded_definition": "G10L17/08 covers the use of distortion metrics or distance measures to quantify the dissimilarity between a probe pattern (e.g., input speech signal) and reference templates during pattern matching for speech recognition tasks. This includes techniques that compare the probe pattern against stored reference templates and compute a distortion or distance score to determine the best match."
                },
                {
                  "symbol": "G10L17/10",
                  "title": "Multimodal systems, i.e. based on the integration of multiple recognition engines or fusion of expert systems",
                  "children": [],
                  "expanded_definition": "G10L17/10 covers multimodal speech recognition systems that integrate or fuse multiple recognition engines or expert systems. These systems combine different modalities or sources of information, such as audio, visual, and contextual data, to improve speech recognition accuracy and performance."
                },
                {
                  "symbol": "G10L17/12",
                  "title": "Score normalisation",
                  "children": [],
                  "expanded_definition": "G10L17/12 - 'Score normalization' covers techniques for normalizing scores or values used in speech recognition pattern matching, such as normalizing likelihood scores or acoustic model scores across different models, time frames, or recognition units like phones or words. This allows for consistent comparison and combination of scores from different sources during speech recognition decoding."
                },
                {
                  "symbol": "G10L17/14",
                  "title": "Use of phonemic categorisation or speech recognition prior to speaker recognition or verification",
                  "children": [],
                  "expanded_definition": "G10L17/14 covers techniques where phonemic categorization (identifying individual speech sounds like vowels and consonants) or speech recognition (converting speech to text) is performed prior to speaker recognition or verification (identifying who is speaking). This involves first processing the speech signal to recognize phonemes or transcribe the speech content, and then using that information to help identify the speaker."
                }
              ],
              "expanded_definition": "G10L17/06 covers techniques specifically related to decision making and pattern matching strategies used for speaker identification or verification. This includes methods for analyzing speech patterns, acoustic features, or other characteristics to determine if a given speech sample matches a known speaker's voice profile or model."
            },
            {
              "symbol": "G10L17/16",
              "title": "Hidden Markov models [HMM]",
              "children": [],
              "expanded_definition": "G10L17/16 covers speech processing techniques specifically using Hidden Markov Models (HMMs) for speaker identification or verification. HMMs are statistical models used to represent and analyze sequential data like speech signals. This category is focused on methods that employ HMMs to model speaker characteristics and recognize or verify the identity of a speaker from their speech."
            },
            {
              "symbol": "G10L17/18",
              "title": "Artificial neural networks Connectionist approaches",
              "children": [],
              "expanded_definition": "G10L17/18 covers techniques for speaker identification or verification that specifically utilize artificial neural networks or connectionist approaches. This includes methods that employ neural network architectures, learning algorithms, and computational models inspired by biological neural networks to analyze and classify speakers based on their voice characteristics."
            },
            {
              "symbol": "G10L17/20",
              "title": "Pattern transformations or operations aimed at increasing system robustness, e.g. against channel noise or different working conditions",
              "children": [],
              "expanded_definition": "G10L17/20 covers techniques for modifying or transforming input speech patterns to increase the robustness of speaker identification/verification systems. This includes methods to handle or compensate for environmental factors like channel noise or variations in recording conditions that could degrade system performance."
            },
            {
              "symbol": "G10L17/22",
              "title": "Interactive procedures Man-machine interfaces",
              "children": [
                {
                  "symbol": "G10L17/24",
                  "title": "the user being prompted to utter a password or a predefined phrase",
                  "children": [],
                  "expanded_definition": "CPC G10L17/24 covers speech recognition systems or techniques where the user is prompted to utter a password or a predefined phrase for authentication, verification, or other security purposes."
                }
              ],
              "expanded_definition": "G10L17/22: This subgroup covers interactive methods and interfaces that enable communication or interaction between a human user and an automated speaker identification or verification system. It involves techniques and procedures for facilitating the interaction between the user and the system during the process of identifying or verifying the speaker's identity."
            },
            {
              "symbol": "G10L17/26",
              "title": "Recognition of special voice characteristics, e.g. for use in lie detectors Recognition of animal voices",
              "children": [],
              "expanded_definition": "G10L17/26 covers techniques for recognizing special voice characteristics, such as those used in lie detectors or for recognizing animal voices."
            }
          ],
          "expanded_definition": "G10L17/00 covers techniques specifically for identifying or verifying speakers based on their voice characteristics or patterns. This includes methods for extracting and matching voice features to enrolled speaker models or profiles for the purpose of determining the speaker's identity or validating their claimed identity."
        },
        {
          "symbol": "G10L19/00",
          "title": "Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis",
          "children": [
            {
              "symbol": "G10L2019/0001",
              "title": "Codebooks",
              "children": [
                {
                  "symbol": "G10L2019/0002",
                  "title": "Codebook adaptations",
                  "children": [],
                  "expanded_definition": "G10L2019/0002 covers techniques for adapting or modifying codebooks used in speech coding systems. This includes methods for updating, refining, or optimizing the codebook entries or structure based on changing conditions, speaker characteristics, or other factors."
                },
                {
                  "symbol": "G10L2019/0003",
                  "title": "Backward prediction of gain",
                  "children": [],
                  "expanded_definition": "G10L2019/0003 covers techniques specifically related to backward prediction of gain in speech or audio coding. This refers to methods where the gain or amplitude of a coded signal is predicted based on previous values or states, rather than computing the gain directly from the current input. The focus is on efficient coding of gain parameters by exploiting temporal redundancies."
                },
                {
                  "symbol": "G10L2019/0004",
                  "title": "Design or structure of the codebook",
                  "children": [
                    {
                      "symbol": "G10L2019/0005",
                      "title": "Multi-stage vector quantisation",
                      "children": [],
                      "expanded_definition": "G10L2019/0005 covers speech or audio coding techniques that specifically utilize multi-stage vector quantization methods for designing or structuring the codebook. Vector quantization involves representing input vectors (speech/audio frames) with entries from a codebook of representative code vectors. In multi-stage vector quantization, this process is carried out in multiple successive stages, rather than a single stage, to improve coding efficiency or quality."
                    },
                    {
                      "symbol": "G10L2019/0006",
                      "title": "Tree or treillis structures; Delayed decisions",
                      "children": [],
                      "expanded_definition": "The CPC category G10L2019/0006 covers codebook designs that use tree or trellis structures, as well as approaches that employ delayed decision techniques. This category specifically focuses on codebook structures and methods that leverage hierarchical (tree) or interconnected (trellis) representations, or techniques that defer decisions or computations related to the codebook to a later stage in the coding/decoding process."
                    }
                  ],
                  "expanded_definition": "G10L2019/0004 covers the specific design or structural details of the codebook used in speech coding or speech recognition systems. This category deals with the organization, construction, and makeup of the codebook itself, which stores and represents the various speech sounds or models used by the system."
                },
                {
                  "symbol": "G10L2019/0007",
                  "title": "Codebook element generation",
                  "children": [
                    {
                      "symbol": "G10L2019/0008",
                      "title": "Algebraic codebooks",
                      "children": [],
                      "expanded_definition": "G10L2019/0008 covers the use of algebraic codebooks for speech coding. Algebraic codebooks are structured codebooks where the codewords have specific algebraic properties or relationships. This can enable more efficient coding or specific desirable characteristics in the coded speech."
                    },
                    {
                      "symbol": "G10L2019/0009",
                      "title": "Orthogonal codebooks",
                      "children": [],
                      "expanded_definition": "G10L 2019/0009 covers the use of orthogonal codebooks in speech recognition systems. Orthogonal codebooks are a type of codebook where the codewords (vectors representing speech features) are mutually orthogonal or linearly independent from each other. This property can improve quantization performance and coding efficiency in certain speech coding applications."
                    },
                    {
                      "symbol": "G10L2019/001",
                      "title": "Interpolation of codebook vectors",
                      "children": [],
                      "expanded_definition": "G10L2019/001 covers techniques specifically related to interpolating or combining codebook vectors in speech coding systems. Codebook vectors are pre-defined sequences used to represent speech signals. Interpolation involves generating new codebook vectors as weighted combinations of existing codebook vectors, rather than using the predefined vectors directly. This allows for a richer representation of speech signals within the constraints of the codebook."
                    }
                  ],
                  "expanded_definition": "G10L2019/0007: 'Codebook element generation' covers techniques and methods for generating the individual elements or entries that make up a codebook used in speech coding or compression."
                },
                {
                  "symbol": "G10L2019/0011",
                  "title": "Long term prediction filters, i.e. pitch estimation",
                  "children": [],
                  "expanded_definition": "G10L2019/0011: 'Long term prediction filters, i.e. pitch estimation' refers to techniques for estimating the fundamental frequency or pitch of a speech signal, typically using long-term prediction filters or analysis of the signal's periodic components over a relatively long time frame."
                },
                {
                  "symbol": "G10L2019/0012",
                  "title": "Smoothing of parameters of the decoder interpolation",
                  "children": [],
                  "expanded_definition": "G10L2019/0012 covers techniques for smoothing or refining the parameters used for decoder interpolation during speech recognition. Decoder interpolation refers to combining multiple acoustic models or language models during the decoding process. This specific category deals with methods to smooth or optimize the parameters involved in interpolating these models, likely to improve recognition accuracy."
                },
                {
                  "symbol": "G10L2019/0013",
                  "title": "Codebook search algorithms",
                  "children": [
                    {
                      "symbol": "G10L2019/0014",
                      "title": "Selection criteria for distances",
                      "children": [],
                      "expanded_definition": "G10L2019/0014 covers selection criteria specifically for selecting distances or similarity measures used in codebook search algorithms for speech or audio coding/decoding. This category is focused on the criteria or methods used to choose the appropriate distance metric or similarity measure when searching codebooks for the best matching codeword or entry during encoding or decoding."
                    },
                    {
                      "symbol": "G10L2019/0015",
                      "title": "Viterbi algorithms",
                      "children": [],
                      "expanded_definition": "G10L2019/0015 covers Viterbi algorithms used specifically for codebook search in speech recognition and processing. The Viterbi algorithm is a dynamic programming algorithm used to find the most likely sequence of hidden states (such as phonemes, words, or other speech units) that could have produced a given observation sequence (e.g. acoustic features extracted from speech). Its application in this category is focused on efficiently searching vector quantization codebooks to find the closest match to the input speech features."
                    }
                  ],
                  "expanded_definition": "G10L2019/0013 covers specific algorithms used for searching through codebooks, which are sets of codes representing speech or audio data. This subgroup is focused on the algorithms and techniques employed to efficiently search and retrieve entries from codebooks during speech coding/decoding processes."
                },
                {
                  "symbol": "G10L2019/0016",
                  "title": "Codebook for LPC parameters",
                  "children": [],
                  "expanded_definition": "G10L2019/0016 covers codebooks specifically designed for coding linear predictive coding (LPC) parameters. LPC is a technique used in speech coding and synthesis to represent the spectral envelope of a speech signal. The codebook in this category provides efficient ways to quantize and encode the LPC parameters, which are the coefficients representing the vocal tract filter, for efficient transmission or storage."
                }
              ],
              "expanded_definition": "G10L2019/0001: This subgroup covers codebooks used for speech or audio coding, specifically related to redundancy reduction techniques such as vocoders or source-filter models. A codebook is a data structure that stores a set of code vectors or codewords used for efficient quantization and compression of speech or audio signals."
            },
            {
              "symbol": "G10L19/0017",
              "title": "Lossless audio signal coding; Perfect reconstruction of coded audio signal by transmission of coding error",
              "children": [],
              "expanded_definition": "G10L19/0017 covers lossless audio signal coding techniques that allow for perfect reconstruction of the original audio signal from the coded data by transmitting any coding error that may occur during the coding process."
            },
            {
              "symbol": "G10L19/0018",
              "title": "Speech coding using phonetic or linguistical decoding of the source; Reconstruction using text-to-speech synthesis",
              "children": [],
              "expanded_definition": "G10L19/0018 covers speech coding techniques that involve decoding the speech signal into phonetic or linguistic units (such as phonemes or words), and then reconstructing the speech signal using text-to-speech synthesis based on the decoded units."
            },
            {
              "symbol": "G10L19/002",
              "title": "Dynamic bit allocation",
              "children": [],
              "expanded_definition": "G10L19/002 covers techniques for dynamically allocating bits or adjusting the bit rate when coding or decoding speech or audio signals using source-filter models or psychoacoustic analysis. This involves adaptively assigning more or fewer bits to different components or frequency bands based on their perceptual importance or other criteria, in order to optimize quality given the available bit rate constraints."
            },
            {
              "symbol": "G10L19/005",
              "title": "Correction of errors induced by the transmission channel, if related to the coding algorithm",
              "children": [],
              "expanded_definition": "G10L19/005 covers techniques for correcting errors in coded speech or audio signals that are specifically caused by issues in the transmission channel, and which are related to the coding/decoding algorithm used. This includes error correction methods that account for or exploit characteristics of the coding algorithm to mitigate transmission errors."
            },
            {
              "symbol": "G10L19/008",
              "title": "Multichannel audio signal coding or decoding using interchannel correlation to reduce redundancy, e.g. joint-stereo, intensity-coding or matrixing",
              "children": [],
              "expanded_definition": "G10L19/008 covers techniques for coding or decoding multichannel audio signals, such as stereo audio, by exploiting interchannel correlation or redundancy between the channels. This includes methods like joint-stereo coding, intensity coding, and audio channel matrixing, which aim to reduce the amount of data needed to represent the multichannel audio while preserving perceptual quality."
            },
            {
              "symbol": "G10L19/012",
              "title": "Comfort noise or silence coding",
              "children": [],
              "expanded_definition": "G10L19/012 covers techniques specifically related to coding or decoding comfort noise or silence in speech or audio coding systems. Comfort noise is a low-level noise signal that is inserted during periods of silence to prevent the complete absence of sound, which can be perceived as an unnatural gap or dropout. This category deals with methods for efficiently encoding and decoding these artificial noise signals representing silence periods."
            },
            {
              "symbol": "G10L19/018",
              "title": "Audio watermarking, i.e. embedding inaudible data in the audio signal",
              "children": [],
              "expanded_definition": "G10L19/018 covers techniques for embedding inaudible data or watermarks into audio signals, without significantly affecting the perceived audio quality."
            },
            {
              "symbol": "G10L19/02",
              "title": "using spectral analysis, e.g. transform vocoders or subband vocoders",
              "children": [
                {
                  "symbol": "G10L19/0204",
                  "title": "using subband decomposition",
                  "children": [
                    {
                      "symbol": "G10L19/0208",
                      "title": "Subband vocoders",
                      "children": [],
                      "expanded_definition": "G10L19/0208: 'Subband vocoders' refers to audio coding techniques that encode the input signal by decomposing it into multiple frequency subbands and applying vocoders (voice coders) separately to each subband. Vocoders model the excitation and spectral envelope of the signal in each subband."
                    }
                  ],
                  "expanded_definition": "G10L19/0204 covers audio coding techniques that specifically use subband decomposition, which involves dividing the input audio signal into multiple frequency subbands or subband signals before encoding or processing each subband separately."
                },
                {
                  "symbol": "G10L19/0212",
                  "title": "using orthogonal transformation",
                  "children": [
                    {
                      "symbol": "G10L19/0216",
                      "title": "using wavelet decomposition",
                      "children": [],
                      "expanded_definition": "G10L19/0216 covers audio coding techniques that specifically utilize wavelet decomposition or wavelet transforms for encoding audio data. Wavelet decomposition involves representing the audio signal using wavelet functions, which provide time-frequency representations. This approach is a particular type of orthogonal transformation applied in audio coding under the broader G10L19/02 category for \"using orthogonal transformation.\""
                    }
                  ],
                  "expanded_definition": "G10L19/0212: This category covers audio coding techniques that specifically use orthogonal transformations, such as discrete cosine transforms (DCT), discrete Fourier transforms (DFT), or wavelet transforms, to convert the audio signal from the time domain to a transformed domain for more efficient encoding."
                },
                {
                  "symbol": "G10L19/022",
                  "title": "Blocking, i.e. grouping of samples in time Choice of analysis windows Overlap factoring",
                  "children": [
                    {
                      "symbol": "G10L19/025",
                      "title": "Detection of transients or attacks for time/frequency resolution switching",
                      "children": [],
                      "expanded_definition": "G10L19/025 covers techniques specifically for detecting transients or signal attacks in audio signals, with the purpose of dynamically switching between different time/frequency resolution analysis configurations to better represent and encode the detected transient/attack portions of the signal."
                    }
                  ],
                  "expanded_definition": "G10L19/022 covers techniques related to blocking or grouping of samples in time domain, specifically:\n\n- Choice of analysis windows: Methods for selecting the type and characteristics of time windows used for spectral analysis of audio signals.\n\n- Overlap factoring: Techniques for determining the amount of overlap between successive time windows, which impacts time resolution and redundancy in the spectral representation.\n\nThis category is focused on windowing and blocking methods applied prior to spectral analysis (e.g., transform, subband decomposition) of audio signals in audio coding."
                },
                {
                  "symbol": "G10L19/028",
                  "title": "Noise substitution, i.e. substituting non-tonal spectral components by noisy source",
                  "children": [],
                  "expanded_definition": "G10L19/028 covers techniques that involve substituting non-tonal or non-harmonic spectral components of an audio signal with noise-like signals or sources. This is done within the context of spectral analysis and encoding methods, such as transform or subband vocoders."
                },
                {
                  "symbol": "G10L19/03",
                  "title": "Spectral prediction for preventing pre-echo Temporary noise shaping [TNS], e.g. in MPEG2 or MPEG4",
                  "children": [],
                  "expanded_definition": "G10L19/03 covers techniques used in audio coding, specifically spectral prediction methods for preventing pre-echo, such as Temporal Noise Shaping (TNS) used in MPEG-2 and MPEG-4 audio coding standards. Pre-echo refers to an audible artifact that can occur with transient signals in transform-based audio coders. TNS involves modifying the quantization noise in the frequency domain to shape it in a way that reduces pre-echo distortion while minimizing overall noise level."
                },
                {
                  "symbol": "G10L19/032",
                  "title": "Quantisation or dequantisation of spectral components",
                  "children": [
                    {
                      "symbol": "G10L19/035",
                      "title": "Scalar quantisation",
                      "children": [],
                      "expanded_definition": "G10L19/035: 'Scalar quantisation' refers to techniques where each individual spectral component or coefficient is quantized independently, without considering the relationships or correlations between different spectral components.\n\nThis covers quantization methods that treat each spectral value as a separate scalar quantity, as opposed to vector quantization techniques which jointly encode and quantize groups of spectral coefficients together."
                    },
                    {
                      "symbol": "G10L19/038",
                      "title": "Vector quantisation, e.g. TwinVQ audio",
                      "children": [],
                      "expanded_definition": "G10L19/038 covers vector quantization techniques specifically applied to the quantization or dequantization of spectral components in audio coding. Vector quantization refers to a data compression technique where input vector values (e.g. sets of spectral coefficients) are mapped to digital codebook indices representing approximations of those vectors. A specific example mentioned is TwinVQ, which is a vector quantization scheme used in certain audio codecs."
                    }
                  ],
                  "expanded_definition": "G10L19/032 covers techniques specifically related to the quantization or dequantization of spectral components in speech or audio coding systems that use spectral analysis, such as transform or subband vocoders. This includes methods for efficiently representing and compressing the spectral components by quantizing their values, as well as algorithms for reconstructing the original spectral data from the quantized values during decoding."
                }
              ],
              "expanded_definition": "G10L19/02 covers techniques for speech or audio coding that specifically use spectral analysis methods, such as transform vocoders or subband vocoders.\n\nTransform vocoders analyze the input signal by applying a transform (e.g., Fourier, wavelet) to separate it into different frequency components or subbands. Subband vocoders directly filter the input into multiple frequency subbands. These techniques code the spectral components or subbands rather than the time-domain signal itself."
            },
            {
              "symbol": "G10L19/04",
              "title": "using predictive techniques",
              "children": [
                {
                  "symbol": "G10L19/06",
                  "title": "Determination or coding of the spectral characteristics, e.g. of the short-term prediction coefficients",
                  "children": [
                    {
                      "symbol": "G10L19/07",
                      "title": "Line spectrum pair [LSP] vocoders",
                      "children": [],
                      "expanded_definition": "G10L19/07 - 'Line spectrum pair [LSP] vocoders' covers voice coding techniques that utilize line spectrum pair (LSP) parameters to represent the spectral envelope or short-term prediction coefficients of the speech signal.\n\nLSP is a way of representing linear prediction coefficients in the frequency domain, often used in voice coding due to its efficient quantization and interpolation properties. LSP vocoders encode the speech by transmitting or storing the LSP parameters along with other parameters like pitch, voicing, and gains."
                    }
                  ],
                  "expanded_definition": "G10L19/06 covers techniques for determining or coding the spectral characteristics of an audio signal, specifically the short-term prediction coefficients used in predictive coding. This includes methods for extracting, quantizing, and encoding the prediction coefficients that represent the spectral envelope of the audio signal over short time frames."
                },
                {
                  "symbol": "G10L19/08",
                  "title": "Determination or coding of the excitation function Determination or coding of the long-term prediction parameters",
                  "children": [
                    {
                      "symbol": "G10L19/083",
                      "title": "the excitation function being an excitation gain",
                      "children": [],
                      "expanded_definition": "The CPC category G10L19/083 covers speech coding techniques where the excitation function used in the speech synthesis or coding process is represented or coded as an excitation gain. This subgroup is specifically focused on methods that determine or code the excitation gain as the excitation function, which is part of the long-term prediction parameters used in speech coding algorithms."
                    },
                    {
                      "symbol": "G10L19/087",
                      "title": "using mixed excitation models, e.g. MELP, MBE, split band LPC or HVXC",
                      "children": [],
                      "expanded_definition": "G10L19/087 covers speech coding techniques that use mixed excitation models. These models combine different types of excitation signals (e.g., periodic and random) to represent the speech signal more accurately. Examples of such mixed excitation models include MELP (Mixed Excitation Linear Prediction), MBE (Multi-Band Excitation), split-band LPC (Linear Predictive Coding), and HVXC (Harmonic Vector Excitation Coding). The key aspect is the use of multiple excitation signals, rather than a single excitation signal, to improve the quality of the coded speech."
                    },
                    {
                      "symbol": "G10L19/09",
                      "title": "Long term prediction, i.e. removing periodical redundancies, e.g. by using adaptive codebook or pitch predictor",
                      "children": [],
                      "expanded_definition": "G10L19/09 covers techniques specifically focused on long-term prediction for speech or audio coding, which involves removing periodic redundancies in the signal. This is achieved by using methods like adaptive codebooks or pitch predictors to exploit the quasi-periodic nature of voiced speech segments."
                    },
                    {
                      "symbol": "G10L19/093",
                      "title": "using sinusoidal excitation models",
                      "children": [],
                      "expanded_definition": "G10L19/093 covers speech coding techniques that use sinusoidal excitation models for determining or coding the excitation function or long-term prediction parameters. Sinusoidal models represent the speech signal as a sum of sinusoids with time-varying amplitudes and frequencies, which can efficiently model the quasi-periodic components of voiced speech."
                    },
                    {
                      "symbol": "G10L19/097",
                      "title": "using prototype waveform decomposition or prototype waveform interpolative [PWI] coders",
                      "children": [],
                      "expanded_definition": "G10L19/097 covers audio coding techniques that use prototype waveform decomposition or prototype waveform interpolation (PWI) coders to determine or code the excitation function or long-term prediction parameters in speech or audio coding."
                    },
                    {
                      "symbol": "G10L19/10",
                      "title": "the excitation function being a multipulse excitation",
                      "children": [
                        {
                          "symbol": "G10L19/107",
                          "title": "Sparse pulse excitation, e.g. by using algebraic codebook",
                          "children": [],
                          "expanded_definition": "G10L19/107 covers speech coding techniques that use sparse pulse excitation, specifically by employing an algebraic codebook. In this context, sparse pulse excitation refers to a method of representing the excitation signal (which models the air flow from the lungs during speech production) using a small number of non-zero pulses at specific locations. The algebraic codebook is a predefined set of sparse pulse vectors that are algebraically constructed to efficiently represent the excitation signal."
                        },
                        {
                          "symbol": "G10L19/113",
                          "title": "Regular pulse excitation",
                          "children": [],
                          "expanded_definition": "G10L19/113 - Regular pulse excitation:\n\nThis category covers speech coding techniques where the excitation function used in the analysis-by-synthesis loop is composed of a regularly spaced series of pulses or impulses. The pulses have a fixed, periodic spacing between them, unlike other multi-pulse excitation techniques that may use irregularly spaced pulses."
                        }
                      ],
                      "expanded_definition": "G10L19/10: This category covers speech coding techniques where the excitation function used in the speech synthesis model is a multipulse excitation, which is a type of excitation signal composed of multiple pulses or impulses within each excitation period."
                    },
                    {
                      "symbol": "G10L19/12",
                      "title": "the excitation function being a code excitation, e.g. in code excited linear prediction [CELP] vocoders",
                      "children": [
                        {
                          "symbol": "G10L19/125",
                          "title": "Pitch excitation, e.g. pitch synchronous innovation CELP [PSI-CELP]",
                          "children": [],
                          "expanded_definition": "G10L19/125: This category covers speech coding techniques where the excitation used in the linear prediction model is generated based on the pitch or fundamental frequency of the speech signal. Specifically, it covers techniques like Pitch Synchronous Innovation Code Excited Linear Prediction (PSI-CELP), where the excitation signal is derived from pitch cycles extracted from the speech signal itself, rather than using a fixed codebook."
                        },
                        {
                          "symbol": "G10L19/13",
                          "title": "Residual excited linear prediction [RELP]",
                          "children": [],
                          "expanded_definition": "G10L19/13 - \"Residual excited linear prediction [RELP]\" covers speech coding techniques that use linear prediction to model the vocal tract and encode the residual signal (the part of the speech signal not represented by the linear prediction filter) using some form of coding or quantization. This is a specific type of code-excited linear prediction (CELP) vocoder."
                        },
                        {
                          "symbol": "G10L19/135",
                          "title": "Vector sum excited linear prediction [VSELP]",
                          "children": [],
                          "expanded_definition": "G10L19/135 covers Vector Sum Excited Linear Prediction (VSELP), which is a specific type of code-excited linear prediction (CELP) coding technique used in speech coding. In VSELP, the excitation signal is generated as a weighted sum of multiple code vectors from a codebook, rather than a single code vector as in traditional CELP."
                        }
                      ],
                      "expanded_definition": "G10L19/12 covers speech/audio coding techniques where the excitation signal used in the linear prediction model is a coded excitation, specifically for Code Excited Linear Prediction (CELP) vocoders. It deals with the determination or coding of the coded excitation used to drive the linear prediction synthesis filter."
                    }
                  ],
                  "expanded_definition": "G10L19/08 covers techniques specifically related to determining or encoding the excitation function, which represents the periodicity and noise components in speech signals during speech coding. It also includes methods for determining or coding the long-term prediction parameters, commonly referred to as pitch or pitch delay, which capture the periodic nature of voiced speech sounds."
                },
                {
                  "symbol": "G10L19/16",
                  "title": "Vocoder architecture",
                  "children": [
                    {
                      "symbol": "G10L19/167",
                      "title": "Audio streaming, i.e. formatting and decoding of an encoded audio signal representation into a data stream for transmission or storage purposes",
                      "children": [],
                      "expanded_definition": "G10L19/167 covers techniques specifically related to formatting and decoding encoded audio signals into a data stream suitable for transmission or storage purposes in audio streaming applications. This category is focused on the processing required to convert an encoded audio representation into a streaming data format that can be transmitted over a network or stored in a file for later playback."
                    },
                    {
                      "symbol": "G10L19/173",
                      "title": "Transcoding, i.e. converting between two coded representations avoiding cascaded coding-decoding",
                      "children": [],
                      "expanded_definition": "G10L19/173 covers methods and systems specifically related to transcoding, which is the process of converting between two different coded representations of audio or speech data, without fully decoding one format and then re-encoding into the other format. This avoids the computational overhead and potential quality loss associated with cascaded coding-decoding operations."
                    },
                    {
                      "symbol": "G10L19/18",
                      "title": "Vocoders using multiple modes",
                      "children": [
                        {
                          "symbol": "G10L19/20",
                          "title": "using sound class specific coding, hybrid encoders or object based coding",
                          "children": [],
                          "expanded_definition": "G10L19/20 covers coding techniques specifically for sound objects or classes of sounds, hybrid encoders that combine multiple coding techniques, or object-based coding where different sound components are encoded separately. This category relates to vocoders that can switch between or combine different coding modes for different sound types or objects within the audio signal."
                        },
                        {
                          "symbol": "G10L19/22",
                          "title": "Mode decision, i.e. based on audio signal content versus external parameters",
                          "children": [],
                          "expanded_definition": "G10L19/22 covers mode selection or mode decision techniques for vocoders that use multiple modes, where the mode is chosen based on the content of the audio signal itself (e.g. speech characteristics, audio features) rather than external parameters (e.g. bit rate, network conditions).\n\nThe key points are:\n\n1) It deals with vocoders operating in multiple modes.\n2) The mode decision/selection is based on analyzing the audio signal content itself.\n3) It excludes mode decisions based solely on external factors like bit rate requirements or network conditions.\n\nSo this category is specifically about intelligently selecting the most appropriate vocoding mode by examining properties of the input audio signal, as opposed to simply reacting to external constraints."
                        },
                        {
                          "symbol": "G10L19/24",
                          "title": "Variable rate codecs, e.g. for generating different qualities using a scalable representation such as hierarchical encoding or layered encoding",
                          "children": [],
                          "expanded_definition": "G10L19/24 covers techniques for variable rate speech codecs that enable generating different qualities of encoded speech using a scalable representation, such as hierarchical encoding or layered encoding. These codecs can dynamically adjust the data rate or quality level of the encoded speech based on factors like available bandwidth or desired quality."
                        }
                      ],
                      "expanded_definition": "G10L19/18 covers vocoders that utilize multiple distinct modes or models for encoding and decoding speech signals. These systems employ a combination of different techniques or representations to analyze and synthesize the input audio, rather than relying on a single mode or model. The multiple modes may correspond to different frequency bands, speech characteristics, or other partitions of the speech signal."
                    }
                  ],
                  "expanded_definition": "G10L19/16 - 'Vocoder architecture': This subgroup covers specific architectures or structural designs of vocoders, which are analysis-synthesis codec systems used to represent speech signals with a low bit rate by encoding the excitation parameters (e.g., pitch, voicing) and spectral envelope parameters of the speech signal. The category is focused on the structural implementation details of different vocoder architectures."
                },
                {
                  "symbol": "G10L19/26",
                  "title": "Pre-filtering or post-filtering",
                  "children": [
                    {
                      "symbol": "G10L19/265",
                      "title": "Pre-filtering, e.g. high frequency emphasis prior to encoding",
                      "children": [],
                      "expanded_definition": "G10L19/265: 'Pre-filtering, e.g. high frequency emphasis prior to encoding' covers techniques that involve filtering the input audio signal before encoding or compression, with a specific focus on emphasizing or boosting high-frequency components of the signal. This pre-processing step is often performed to improve the coding efficiency or quality of the subsequent audio encoding or compression process."
                    }
                  ],
                  "expanded_definition": "G10L19/26 covers techniques used specifically for pre-filtering or post-filtering audio signals in predictive audio coding methods. This includes any filtering operations performed on the input audio signal before predictive encoding, or on the decoded audio signal after predictive decoding, in order to improve coding efficiency or perceptual quality."
                }
              ],
              "expanded_definition": "G10L19/04 covers methods that use predictive techniques for coding or decoding speech or audio signals when applying source filter models or psychoacoustic analysis for redundancy reduction (e.g. in vocoders). This includes techniques that predict future speech/audio signal characteristics or parameters to improve coding efficiency."
            }
          ],
          "expanded_definition": "G10L19/00 covers techniques and methods specifically focused on coding or decoding of speech or audio signals using source-filter models or psychoacoustic analysis for the purpose of redundancy reduction, such as in vocoders.\n\nThis category deals with algorithms and approaches that analyze and synthesize speech or audio signals by modeling the source (e.g., vocal tract excitation) and filter (e.g., vocal tract resonances) components, as well as techniques that leverage psychoacoustic principles related to human auditory perception, with the goal of efficiently encoding the signals by removing redundant information."
        },
        {
          "symbol": "G10L21/00",
          "title": "Speech or voice signal processing techniques to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility",
          "children": [
            {
              "symbol": "G10L21/003",
              "title": "Changing voice quality, e.g. pitch or formants",
              "children": [
                {
                  "symbol": "G10L21/007",
                  "title": "characterised by the process used",
                  "children": [
                    {
                      "symbol": "G10L21/01",
                      "title": "Correction of time axis",
                      "children": [],
                      "expanded_definition": "G10L21/01 'Correction of time axis' covers techniques specifically related to adjusting or modifying the time scale or time axis of an audio signal. This can include time compression, time stretching, time warping, or other transformations that alter the duration or timing of the audio signal without affecting its pitch or spectral characteristics."
                    },
                    {
                      "symbol": "G10L21/013",
                      "title": "Adapting to target pitch",
                      "children": [
                        {
                          "symbol": "G10L2021/0135",
                          "title": "Voice conversion or morphing",
                          "children": [],
                          "expanded_definition": "G10L2021/0135: This category covers techniques specifically related to voice conversion or morphing, where characteristics of a source speaker's voice are transformed or morphed to match the voice characteristics of a target speaker. This can involve modifying aspects such as pitch, timbre, prosody, and other vocal qualities to achieve the desired voice conversion or morphing effect."
                        }
                      ],
                      "expanded_definition": "G10L21/013 - 'Adapting to target pitch' covers techniques and methods for adjusting or modifying speech recognition systems to accommodate differences in the pitch (fundamental frequency) of the target speaker's voice. This could involve adapting acoustic models, pitch tracking algorithms, or other components to better handle speakers with higher or lower pitched voices compared to the training data."
                    }
                  ],
                  "expanded_definition": "G10L21/007 covers speech processing techniques specifically characterized by the processes used for changing voice quality parameters such as pitch or formants, rather than being defined by the intended application or effect on the voice signal."
                }
              ],
              "expanded_definition": "G10L21/003 covers techniques for changing voice quality characteristics such as pitch or formants when processing speech or voice signals."
            },
            {
              "symbol": "G10L21/02",
              "title": "Speech enhancement, e.g. noise reduction or echo cancellation",
              "children": [
                {
                  "symbol": "G10L21/0208",
                  "title": "Noise filtering",
                  "children": [
                    {
                      "symbol": "G10L2021/02082",
                      "title": "the noise being echo, reverberation of the speech",
                      "children": [],
                      "expanded_definition": "G10L2021/02082: Covers techniques for filtering or suppressing echo and reverberation in speech signals specifically, which are types of noise caused by sound reflections from surfaces in enclosed spaces."
                    },
                    {
                      "symbol": "G10L2021/02085",
                      "title": "Periodic noise",
                      "children": [],
                      "expanded_definition": "G10L2021/02085: 'Periodic noise' covers techniques specifically for filtering or removing periodic or cyclical noise signals from audio data. This includes methods for identifying and suppressing noise components that exhibit a repeating or oscillating pattern, such as hums, buzzes, or other recurring disturbances."
                    },
                    {
                      "symbol": "G10L2021/02087",
                      "title": "the noise being separate speech, e.g. cocktail party",
                      "children": [],
                      "expanded_definition": "G10L2021/02087 covers techniques for filtering or separating speech noise from the desired speech signal, where the noise is caused by separate speech signals, such as in a cocktail party scenario with multiple speakers or conversations occurring simultaneously."
                    },
                    {
                      "symbol": "G10L21/0216",
                      "title": "characterised by the method used for estimating noise",
                      "children": [
                        {
                          "symbol": "G10L2021/02161",
                          "title": "Number of inputs available containing the signal or the noise to be suppressed",
                          "children": [
                            {
                              "symbol": "G10L2021/02163",
                              "title": "Only one microphone",
                              "children": [],
                              "expanded_definition": "G10L2021/02163 covers speech recognition or speaker verification systems that use only a single microphone input to capture the audio signal containing both the desired speech and noise that needs to be suppressed."
                            },
                            {
                              "symbol": "G10L2021/02165",
                              "title": "Two microphones, one receiving mainly the noise signal and the other one mainly the speech signal",
                              "children": [],
                              "expanded_definition": "G10L2021/02165 covers systems or methods that use two microphones, where one microphone primarily captures noise signals, and the other microphone primarily captures speech signals. This configuration enables the system to distinguish and process the speech and noise components separately for noise suppression or speech enhancement purposes."
                            },
                            {
                              "symbol": "G10L2021/02166",
                              "title": "Microphone arrays; Beamforming",
                              "children": [],
                              "expanded_definition": "G10L2021/02166 covers microphone arrays and beamforming techniques specifically for applications where multiple input signals containing the desired speech signal and noise are available. It relates to methods and systems for processing the multiple input signals to enhance the target speech signal while suppressing noise or interference through spatial filtering and directional signal processing enabled by the microphone array configuration."
                            }
                          ],
                          "expanded_definition": "G10L2021/02161 covers speech recognition methods where the noise suppression technique uses multiple input signals containing both the desired speech signal and the noise to be suppressed. This allows the system to leverage information from the different input signals to better estimate and remove the unwanted noise components."
                        },
                        {
                          "symbol": "G10L2021/02168",
                          "title": "the estimation exclusively taking place during speech pauses",
                          "children": [],
                          "expanded_definition": "G10L2021/02168 covers techniques for estimating noise during speech pauses or silence periods, without considering noise estimation during actual speech segments."
                        },
                        {
                          "symbol": "G10L21/0224",
                          "title": "Processing in the time domain",
                          "children": [],
                          "expanded_definition": "G10L21/0224: 'Processing in the time domain' refers to noise estimation techniques that operate on the speech signal directly in the time domain, rather than transforming the signal to another domain (e.g., frequency domain). These methods analyze and estimate the noise characteristics from the time-domain waveform of the speech signal itself."
                        },
                        {
                          "symbol": "G10L21/0232",
                          "title": "Processing in the frequency domain",
                          "children": [],
                          "expanded_definition": "G10L21/0232: 'Processing in the frequency domain' covers noise estimation methods that operate by analyzing the input signal in the frequency domain, such as through techniques like spectral subtraction or Wiener filtering applied to the frequency components of the noisy signal."
                        }
                      ],
                      "expanded_definition": "G10L21/0216: This category covers methods specifically focused on estimating or determining the noise characteristics in an audio signal. It encompasses techniques for analyzing and modeling the noise present in the input audio, which can then be used for effective noise filtering or speech enhancement. The key aspect is the approach used to estimate the noise component, rather than the noise filtering itself."
                    },
                    {
                      "symbol": "G10L21/0264",
                      "title": "characterised by the type of parameter measurement, e.g. correlation techniques, zero crossing techniques or predictive techniques",
                      "children": [],
                      "expanded_definition": "G10L21/0264 covers methods for noise filtering in speech processing that are characterized by the specific type of parameter measurement techniques used, such as correlation techniques, zero-crossing techniques, or predictive techniques.\n\nThis category is focused on the particular signal analysis and parameter measurement approaches employed for noise filtering, rather than on the general noise filtering methodologies themselves."
                    }
                  ],
                  "expanded_definition": "G10L21/0208 covers techniques specifically related to filtering or removing noise from speech signals. This includes methods for identifying and suppressing background noise, environmental noise, or any other unwanted audio components that are not part of the intended speech."
                },
                {
                  "symbol": "G10L21/0272",
                  "title": "Voice signal separating",
                  "children": [
                    {
                      "symbol": "G10L21/028",
                      "title": "using properties of sound source",
                      "children": [],
                      "expanded_definition": "G10L21/028 covers techniques for separating voice signals using properties or characteristics of the sound source itself, such as the direction, location, or movement of the source. This can involve techniques like beamforming, source localization, or using the differences between sound sources to facilitate separation and extraction of the desired voice signal."
                    },
                    {
                      "symbol": "G10L21/0308",
                      "title": "characterised by the type of parameter measurement, e.g. correlation techniques, zero crossing techniques or predictive techniques",
                      "children": [],
                      "expanded_definition": "G10L21/0308: This category covers techniques for voice signal separation characterized by the specific type of parameter measurement used, such as correlation techniques, zero crossing techniques, or predictive techniques, to separate one or more voice signals from a combined audio signal."
                    }
                  ],
                  "expanded_definition": "G10L21/0272 covers techniques specifically for separating or extracting individual voice signals from a mixed audio signal containing multiple voices or speakers."
                },
                {
                  "symbol": "G10L21/0316",
                  "title": "by changing the amplitude",
                  "children": [
                    {
                      "symbol": "G10L21/0324",
                      "title": "Details of processing therefor",
                      "children": [
                        {
                          "symbol": "G10L21/0332",
                          "title": "involving modification of waveforms",
                          "children": [],
                          "expanded_definition": "G10L21/0332 covers techniques that involve modifying or altering the waveform (time-domain representation) of an audio signal during speech processing."
                        },
                        {
                          "symbol": "G10L21/034",
                          "title": "Automatic adjustment",
                          "children": [],
                          "expanded_definition": "G10L21/034: This covers techniques for automatically adjusting or adapting aspects of speech processing, such as adjusting parameters or models used for speech recognition, synthesis, coding, or enhancement. The automatic adjustment aims to improve performance or adapt to changing conditions without manual intervention."
                        }
                      ],
                      "expanded_definition": "G10L21/0324 covers technical details and processing steps specifically related to speech coding or compression by modifying the amplitude of the speech signal. This category is limited to the particular techniques and algorithms used for amplitude modification as part of the speech coding process."
                    },
                    {
                      "symbol": "G10L21/0356",
                      "title": "for synchronising with other signals, e.g. video signals",
                      "children": [],
                      "expanded_definition": "G10L21/0356: This covers speech coding techniques for synchronizing speech signals with other signals, such as video signals, by adjusting the amplitude of the speech signal."
                    },
                    {
                      "symbol": "G10L21/0364",
                      "title": "for improving intelligibility",
                      "children": [
                        {
                          "symbol": "G10L2021/03643",
                          "title": "Diver speech",
                          "children": [],
                          "expanded_definition": "G10L2021/03643 'Diver speech': This category covers techniques specifically designed to improve intelligibility of speech uttered by divers underwater or in other environments with interference or distortion caused by the surrounding water medium."
                        },
                        {
                          "symbol": "G10L2021/03646",
                          "title": "Stress or Lombard effect",
                          "children": [],
                          "expanded_definition": "The CPC category G10L2021/03646 'Stress or Lombard effect' covers techniques for improving speech intelligibility by accounting for or compensating the acoustic changes that occur in speech when the speaker is under physical or psychological stress, such as speaking with increased vocal effort in noisy environments (known as the Lombard effect)."
                        }
                      ],
                      "expanded_definition": "G10L21/0364 - Covers techniques for enhancing the intelligibility or clarity of speech by modifying the amplitude or volume levels of the audio signal, such as amplitude compression, dynamic range adjustment, or volume normalization."
                    }
                  ],
                  "expanded_definition": "G10L21/0316: Speech enhancement techniques that operate by changing or modifying the amplitude or volume levels of the speech signal itself, as opposed to techniques that modify other characteristics like frequency or phase. This could include amplitude normalization, dynamic range compression/expansion, or adaptive gain control applied specifically for noise reduction or speech enhancement purposes."
                },
                {
                  "symbol": "G10L21/038",
                  "title": "using band spreading techniques",
                  "children": [
                    {
                      "symbol": "G10L21/0388",
                      "title": "Details of processing therefor",
                      "children": [],
                      "expanded_definition": "G10L21/0388 covers specific details or techniques related to processing speech or audio signals using band spreading techniques, such as how the spreading is performed, specific algorithms or methods employed for the spreading operation, or any other implementation details specific to the band spreading process itself."
                    }
                  ],
                  "expanded_definition": "G10L21/038 covers speech enhancement techniques that specifically employ band spreading methods. Band spreading refers to techniques that spread or distribute the frequency components of a signal across a broader frequency range. This can help reduce perceived noise or interference in certain frequency bands by redistributing the speech energy to other less affected bands."
                }
              ],
              "expanded_definition": "G10L21/02 covers techniques specifically for speech enhancement, such as noise reduction or echo cancellation, applied to speech or voice signals to improve their quality or intelligibility."
            },
            {
              "symbol": "G10L21/04",
              "title": "Time compression or expansion",
              "children": [
                {
                  "symbol": "G10L21/043",
                  "title": "by changing speed",
                  "children": [
                    {
                      "symbol": "G10L21/045",
                      "title": "using thinning out or insertion of a waveform",
                      "children": [
                        {
                          "symbol": "G10L21/047",
                          "title": "characterised by the type of waveform to be thinned out or inserted",
                          "children": [],
                          "expanded_definition": "G10L21/047 covers speech coding methods and systems that are specifically characterized by the type of waveform used for thinning out (removing samples) or inserting (adding samples) within the speech waveform during coding/decoding."
                        },
                        {
                          "symbol": "G10L21/049",
                          "title": "characterised by the interconnection of waveforms",
                          "children": [],
                          "expanded_definition": "G10L21/049 covers speech coding techniques specifically characterized by the interconnection or linking of waveforms during the thinning out (removing samples) or insertion (adding samples) processes to modify the speech waveform."
                        }
                      ],
                      "expanded_definition": "G10L21/045 covers techniques for changing the speed of speech or audio signals by either removing (thinning out) or adding (inserting) portions of the waveform itself. This allows the playback speed to be adjusted without changing the pitch or introducing other distortions associated with basic time-stretching methods."
                    }
                  ],
                  "expanded_definition": "G10L21/043: This category covers techniques for time compression or expansion of audio signals by specifically changing the playback speed, either increasing it for compression or decreasing it for expansion. The focus is on methods that alter the temporal characteristics of the audio by modifying the speed at which it is reproduced, while aiming to preserve the pitch and other signal properties."
                },
                {
                  "symbol": "G10L21/055",
                  "title": "for synchronising with other signals, e.g. video signals",
                  "children": [],
                  "expanded_definition": "G10L21/055 covers techniques for synchronizing time-compressed or time-expanded speech signals with other signals, such as video signals. This category specifically deals with aligning the timing or synchronization of temporally modified speech with corresponding multimedia content like video."
                },
                {
                  "symbol": "G10L21/057",
                  "title": "for improving intelligibility",
                  "children": [
                    {
                      "symbol": "G10L2021/0575",
                      "title": "Aids for the handicapped in speaking",
                      "children": [],
                      "expanded_definition": "G10L2021/0575 covers speech synthesis techniques specifically designed to aid individuals with speech or speaking disabilities. This includes methods for generating synthetic speech that is more intelligible or comprehensible for those with certain handicaps or impairments affecting their ability to produce natural speech."
                    }
                  ],
                  "expanded_definition": "G10L21/057: This category covers techniques specifically aimed at improving the intelligibility or clarity of speech when performing time compression or expansion. The methods classified here are intended to enhance the understandability of time-compressed or time-expanded speech, making it easier to perceive and comprehend."
                }
              ],
              "expanded_definition": "G10L21/04 covers techniques specifically focused on altering the time dimension of a speech or voice signal to achieve either time compression (speeding up) or time expansion (slowing down) of the audible signal."
            },
            {
              "symbol": "G10L21/06",
              "title": "Transformation of speech into a non-audible representation, e.g. speech visualisation or speech processing for tactile aids",
              "children": [
                {
                  "symbol": "G10L2021/065",
                  "title": "Aids for the handicapped in understanding",
                  "children": [],
                  "expanded_definition": "G10L2021/065 covers techniques specifically aimed at aiding individuals with disabilities or impairments in understanding speech or speech representations. This may include methods for visually displaying speech information for those with hearing impairments, or tactile aids that convey speech through means other than audio for users who are deaf or hard of hearing."
                },
                {
                  "symbol": "G10L21/10",
                  "title": "Transforming into visible information",
                  "children": [
                    {
                      "symbol": "G10L2021/105",
                      "title": "Synthesis of the lips movements from speech, e.g. for talking heads",
                      "children": [],
                      "expanded_definition": "G10L2021/105 covers techniques for synthesizing or generating visible lip movements from speech signals, typically for creating animated talking heads or lip-synced virtual characters."
                    },
                    {
                      "symbol": "G10L21/12",
                      "title": "by displaying time domain information",
                      "children": [],
                      "expanded_definition": "G10L21/12 covers techniques for transforming speech or audio signals into visible information by displaying representations in the time domain, such as waveforms or other time-based visualizations of the audio signal."
                    },
                    {
                      "symbol": "G10L21/14",
                      "title": "by displaying frequency domain information",
                      "children": [],
                      "expanded_definition": "G10L21/14: This category covers techniques for displaying or visualizing speech or audio signals in the frequency domain, such as displaying a spectrogram, which shows the frequency content of the signal over time. It specifically encompasses the visual representation of the signal's frequency components or spectrum as opposed to the time-domain waveform."
                    }
                  ],
                  "expanded_definition": "G10L21/10 covers techniques for transforming speech into a visible representation or visual information, such as displaying speech waveforms, spectrograms, or other visual aids for speech visualization."
                },
                {
                  "symbol": "G10L21/16",
                  "title": "Transforming into a non-visible representation",
                  "children": [],
                  "expanded_definition": "G10L21/16 covers techniques for transforming speech signals into non-visible representations, such as converting audio data into visual formats like spectrograms, waveform displays, or other graphical representations."
                },
                {
                  "symbol": "G10L21/18",
                  "title": "Details of the transformation process",
                  "children": [],
                  "expanded_definition": "G10L21/18 'Details of the transformation process' covers specific aspects or techniques used in transforming speech into a non-audible representation, such as visualization or tactile representation of speech. This could include algorithms, feature extraction methods, mapping techniques, or other details related to the core transformation process itself."
                }
              ],
              "expanded_definition": "G10L21/06 covers techniques for transforming speech signals into a non-audible representation, such as visual displays or tactile aids, without modifying the speech signal itself."
            }
          ],
          "expanded_definition": "G10L21/00 covers techniques for processing speech or voice signals to produce another audible signal (e.g., modifying speech quality or intelligibility) or a non-audible signal (e.g., visual or tactile representation of speech) by transforming or modifying the original speech signal."
        },
        {
          "symbol": "G10L25/00",
          "title": "Speech or voice analysis techniques not restricted to a single one of groups",
          "children": [
            {
              "symbol": "G10L25/03",
              "title": "characterised by the type of extracted parameters",
              "children": [
                {
                  "symbol": "G10L25/06",
                  "title": "the extracted parameters being correlation coefficients",
                  "children": [],
                  "expanded_definition": "G10L25/06 covers speech recognition systems where the extracted parameters used for speech processing are correlation coefficients. Correlation coefficients measure the statistical relationship or dependence between two variables, quantifying how they co-vary relative to one another. In this context, the speech recognition system extracts correlation coefficients from the input speech signal as parametric representations for further processing and recognition."
                },
                {
                  "symbol": "G10L25/09",
                  "title": "the extracted parameters being zero crossing rates",
                  "children": [],
                  "expanded_definition": "G10L25/09: This category covers techniques for speech processing where the extracted speech parameters are zero crossing rates. A zero crossing rate represents the number of times the speech signal crosses the zero amplitude level within a certain time frame, providing information about the high-frequency content of the speech signal."
                },
                {
                  "symbol": "G10L25/12",
                  "title": "the extracted parameters being prediction coefficients",
                  "children": [],
                  "expanded_definition": "G10L25/12 covers techniques where the extracted parameters used for speech processing or coding are prediction coefficients. Prediction coefficients are a type of parameter that represent the predictive relationship between samples in a speech signal, which can be used for efficient encoding or modeling of the speech data."
                },
                {
                  "symbol": "G10L25/15",
                  "title": "the extracted parameters being formant information",
                  "children": [],
                  "expanded_definition": "G10L25/15 covers speech recognition techniques where the extracted parameters used for recognition are specifically formant information. Formants are resonant frequencies that characterize the different sound patterns produced by the human vocal tract, and formant information encapsulates details about these resonances."
                },
                {
                  "symbol": "G10L25/18",
                  "title": "the extracted parameters being spectral information of each sub-band",
                  "children": [],
                  "expanded_definition": "G10L25/18: This category covers speech recognition techniques where the extracted parameters used for recognizing speech are spectral information derived from sub-bands or frequency bands of the input speech signal.\n\nIn simpler terms, it refers to speech recognition methods that analyze the spectral or frequency content of the speech signal in different sub-bands or frequency regions, and use this sub-band spectral information as the feature parameters for recognition."
                },
                {
                  "symbol": "G10L25/21",
                  "title": "the extracted parameters being power information",
                  "children": [],
                  "expanded_definition": "G10L25/21 covers speech recognition systems where the extracted parameters used for recognition are related to power information, such as energy levels or amplitude values from the input speech signal."
                },
                {
                  "symbol": "G10L25/24",
                  "title": "the extracted parameters being the cepstrum",
                  "children": [],
                  "expanded_definition": "G10L25/24 covers speech or audio signal processing techniques where the extracted parameters used for characterizing the signal are cepstral coefficients, which represent the cepstrum of the signal. The cepstrum is a nonlinear operation on a signal, computed as the inverse Fourier transform of the logarithm of the signal's Fourier transform magnitude."
                }
              ],
              "expanded_definition": "G10L25/03 covers speech or voice analysis techniques characterized by the specific type of parameters extracted from the speech or voice signal. This includes techniques that analyze and extract particular acoustic features, spectral parameters, prosodic features, or other characteristic parameters from the input speech or voice data."
            },
            {
              "symbol": "G10L25/27",
              "title": "characterised by the analysis technique",
              "children": [
                {
                  "symbol": "G10L25/30",
                  "title": "using neural networks",
                  "children": [],
                  "expanded_definition": "G10L25/30 (using neural networks): This category covers speech recognition techniques that utilize neural networks, which are computing systems inspired by biological neural networks in the brain. Specifically, it encompasses methods that employ artificial neural networks, such as feedforward neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), or other types of neural network architectures, for analyzing and recognizing speech signals."
                },
                {
                  "symbol": "G10L25/33",
                  "title": "using fuzzy logic",
                  "children": [],
                  "expanded_definition": "G10L 25/33 covers speech recognition techniques that utilize fuzzy logic. Fuzzy logic is a form of multi-valued logic that allows for intermediate truth values between binary true and false states. In speech recognition, fuzzy logic can be applied to handle uncertainties and ambiguities inherent in speech data and modeling."
                },
                {
                  "symbol": "G10L25/36",
                  "title": "using chaos theory",
                  "children": [],
                  "expanded_definition": "G10L25/36 covers speech recognition techniques that utilize chaos theory. Chaos theory refers to the study of complex, nonlinear systems that exhibit seemingly random behavior but follow deterministic laws with high sensitivity to initial conditions. This subgroup likely includes speech recognition methods that model and analyze the chaotic dynamics present in speech signals using chaos theory principles and techniques."
                },
                {
                  "symbol": "G10L25/39",
                  "title": "using genetic algorithms",
                  "children": [],
                  "expanded_definition": "G10L25/39 covers speech recognition techniques that specifically use genetic algorithms for analyzing and recognizing speech. Genetic algorithms are a type of optimization algorithm inspired by the process of natural selection, where a population of candidate solutions evolves towards better solutions over successive generations through processes like mutation, crossover, and selection based on a fitness function. This subgroup focuses on applying such genetic algorithm approaches to speech analysis and recognition tasks."
                }
              ],
              "expanded_definition": "G10L25/27 covers speech or voice analysis techniques specifically characterized by the particular analysis technique employed, such as techniques involving statistical modeling, neural networks, or other specific algorithmic approaches for analyzing speech or voice signals."
            },
            {
              "symbol": "G10L25/45",
              "title": "characterised by the type of analysis window",
              "children": [],
              "expanded_definition": "G10L25/45 covers speech or voice analysis techniques characterized by the type of analysis window used. This category specifically focuses on techniques that employ a particular type of window function or windowing approach when analyzing speech or voice signals."
            },
            {
              "symbol": "G10L25/48",
              "title": "specially adapted for particular use",
              "children": [
                {
                  "symbol": "G10L25/51",
                  "title": "for comparison or discrimination",
                  "children": [
                    {
                      "symbol": "G10L25/54",
                      "title": "for retrieval",
                      "children": [],
                      "expanded_definition": "G10L25/54: Covers speech recognition techniques specifically for retrieving information or data, such as retrieving documents, files, or records based on spoken queries or commands."
                    },
                    {
                      "symbol": "G10L25/57",
                      "title": "for processing of video signals",
                      "children": [],
                      "expanded_definition": "G10L25/57: This category covers techniques for processing video signals specifically for the purpose of speech recognition, such as extracting visual speech features from video data of a speaker's lip movements or facial expressions to aid in speech recognition or discrimination."
                    },
                    {
                      "symbol": "G10L25/60",
                      "title": "for measuring the quality of voice signals",
                      "children": [],
                      "expanded_definition": "G10L25/60 covers techniques specifically for measuring the quality of voice signals in the context of speech recognition or processing. This includes methods to evaluate characteristics like clarity, noise levels, distortion, or other factors that impact the quality or intelligibility of voice input signals."
                    },
                    {
                      "symbol": "G10L25/63",
                      "title": "for estimating an emotional state",
                      "children": [],
                      "expanded_definition": "G10L25/63 covers speech recognition techniques specifically focused on estimating an emotional state from the input speech signal. This involves analyzing acoustic features of the speech to detect and classify emotions like happiness, anger, sadness, etc. expressed by the speaker."
                    },
                    {
                      "symbol": "G10L25/66",
                      "title": "for extracting parameters related to health condition",
                      "children": [],
                      "expanded_definition": "G10L25/66 covers speech signal processing techniques specifically for extracting parameters related to a speaker's health condition from their speech. This includes methods to analyze vocal characteristics and extract features indicative of various health issues or disorders based on variations in speech patterns, voice quality, articulation, etc."
                    }
                  ],
                  "expanded_definition": "G10L25/51 covers speech recognition systems or techniques adapted specifically for use in comparing or discriminating between speech inputs. This could include methods for comparing a spoken utterance against stored speech models to identify the uttered content, or discriminating between different speakers, accents, languages, etc. based on characteristics of the input speech signal."
                },
                {
                  "symbol": "G10L25/69",
                  "title": "for evaluating synthetic or decoded voice signals",
                  "children": [],
                  "expanded_definition": "The CPC category G10L25/69 covers techniques specifically adapted for evaluating synthetic or decoded voice signals. This includes methods for assessing the quality, naturalness, or intelligibility of voice signals that have been artificially generated (such as by text-to-speech systems) or reconstructed from an encoded representation (such as after speech coding/decoding). The key focus is on evaluation techniques tailored for these types of non-natural voice signals."
                },
                {
                  "symbol": "G10L25/72",
                  "title": "for transmitting results of analysis",
                  "children": [],
                  "expanded_definition": "G10L25/72 covers techniques specifically adapted for transmitting the results of speech or audio analysis over a communication channel or network. This includes methods for encoding, packetizing, and transmitting analysis data like recognized speech, speaker characteristics, or audio event detection results for applications like voice interfaces, speech analytics, or audio monitoring systems."
                }
              ],
              "expanded_definition": "G10L25/48 covers speech or voice analysis techniques that are specially adapted or designed for a particular use or application, beyond the general speech/voice analysis covered by the broader G10L25/00 group.\n\nThis subgroup is intended to capture speech/voice analysis methods tailored or optimized for specific use cases or domains, rather than being general-purpose techniques."
            },
            {
              "symbol": "G10L25/75",
              "title": "for modelling vocal tract parameters",
              "children": [],
              "expanded_definition": "G10L25/75: Speech or voice analysis techniques specifically for modeling the parameters of the vocal tract.\n\nThis category covers methods and algorithms used to analyze speech or voice signals in order to estimate and model the characteristics of the speaker's vocal tract, such as vocal tract area functions, vocal tract transfer functions, articulatory parameters, or other related parameters that describe the physical configuration of the vocal tract during speech production."
            },
            {
              "symbol": "G10L25/78",
              "title": "Detection of presence or absence of voice signals",
              "children": [
                {
                  "symbol": "G10L2025/783",
                  "title": "based on threshold decision",
                  "children": [
                    {
                      "symbol": "G10L2025/786",
                      "title": "Adaptive threshold",
                      "children": [],
                      "expanded_definition": "G10L2025/786 (Adaptive threshold): This category covers speech recognition techniques that use an adaptive or dynamically adjustable threshold for making decisions, such as determining whether a speech segment corresponds to a particular acoustic unit (e.g., phoneme, word). The threshold value is not fixed but is updated or adapted based on certain criteria or conditions during the recognition process."
                    }
                  ],
                  "expanded_definition": "G10L2025/783 covers voice presence or absence detection methods that involve making a decision based on comparing a parameter or feature to a threshold value."
                },
                {
                  "symbol": "G10L25/81",
                  "title": "for discriminating voice from music",
                  "children": [],
                  "expanded_definition": "G10L25/81 covers techniques specifically for discriminating or distinguishing between voice (speech) and music signals when detecting the presence or absence of voice signals."
                },
                {
                  "symbol": "G10L25/84",
                  "title": "for discriminating voice from noise",
                  "children": [],
                  "expanded_definition": "G10L25/84 covers techniques specifically for discriminating or separating voice signals from noise or other non-voice audio signals within the context of detecting the presence or absence of voice."
                },
                {
                  "symbol": "G10L25/87",
                  "title": "Detection of discrete points within a voice signal",
                  "children": [],
                  "expanded_definition": "G10L25/87 covers techniques specifically for detecting discrete points or events within a voice signal. This includes methods for locating and identifying specific time points of interest in the voice data, such as the start and end points of speech segments, word boundaries, or other acoustic events like plosives or fricatives."
                }
              ],
              "expanded_definition": "G10L25/78 covers techniques specifically for detecting the presence or absence of voice signals or speech in an audio input, without performing any further speech or voice analysis."
            },
            {
              "symbol": "G10L25/90",
              "title": "Pitch determination of speech signals",
              "children": [
                {
                  "symbol": "G10L2025/903",
                  "title": "using a laryngograph",
                  "children": [],
                  "expanded_definition": "G10L2025/903: This category covers techniques for determining the pitch or fundamental frequency of speech signals using a laryngograph, which is a device that monitors the movement of the larynx and vocal folds during speech production by measuring changes in electrical impedance across the neck."
                },
                {
                  "symbol": "G10L2025/906",
                  "title": "Pitch tracking",
                  "children": [],
                  "expanded_definition": "G10L2025/906 - Pitch tracking: This subgroup covers techniques specifically focused on tracking the continuous variation or contour of the fundamental frequency (pitch) over time in speech signals."
                }
              ],
              "expanded_definition": "G10L25/90: 'Pitch determination of speech signals' covers techniques specifically focused on determining or estimating the fundamental frequency or pitch of speech signals. This includes methods for pitch tracking, pitch extraction, and pitch detection from audio signals containing human speech."
            },
            {
              "symbol": "G10L25/93",
              "title": "Discriminating between voiced and unvoiced parts of speech signals",
              "children": [
                {
                  "symbol": "G10L2025/932",
                  "title": "Decision in previous or following frames",
                  "children": [],
                  "expanded_definition": "G10L2025/932 covers techniques for speech recognition that involve making decisions based on information or analysis from previous or following (neighboring) speech frames, in order to discriminate between voiced and unvoiced parts of the speech signal.\n\nThis category specifically deals with using contextual information from temporally adjacent frames, rather than just analyzing each frame in isolation, when determining if a speech segment contains voiced or unvoiced sounds."
                },
                {
                  "symbol": "G10L2025/935",
                  "title": "Mixed voiced class; Transitions",
                  "children": [],
                  "expanded_definition": "G10L2025/935 covers techniques for classifying and detecting mixed voiced speech segments or transitions between voiced and unvoiced speech segments in an audio signal."
                },
                {
                  "symbol": "G10L2025/937",
                  "title": "Signal energy in various frequency bands",
                  "children": [],
                  "expanded_definition": "G10L2025/937 covers techniques for discriminating between voiced and unvoiced speech by analyzing the signal energy present in various frequency bands.\n\nThis category is specifically focused on methods that exploit the differences in energy distribution across different frequency bands to distinguish voiced speech sounds (with periodic excitation from vocal folds) from unvoiced speech sounds (with aperiodic excitation through turbulent airflow)."
                }
              ],
              "expanded_definition": "G10L25/93 covers techniques specifically for discriminating or distinguishing between the voiced and unvoiced portions or segments of speech signals.\n\nVoiced speech refers to portions produced with vibration of the vocal cords, while unvoiced speech lacks vocal cord vibration. This category deals with methods to differentiate and separate the voiced and unvoiced components within a speech signal."
            }
          ],
          "expanded_definition": "G10L25/00 covers speech or voice analysis techniques that are not restricted to a single one of the subgroups under G10L 25/00. These techniques involve analyzing various aspects of speech or voice signals, but cannot be classified into just one of the more specific subgroups defined under G10L25/00."
        },
        {
          "symbol": "G10L99/00",
          "title": "Subject matter not provided for in other groups of this subclass",
          "children": [],
          "expanded_definition": "G10L 99/00 covers subject matter related to speech or audio signal processing not provided for in other groups within the G10L subclass.\n\nThis residual category is intended to capture any speech/audio processing techniques, systems, applications, etc. that do not fit within the scope of the more specific G10L groups covering areas like speech recognition, speaker recognition, speech coding, etc."
        }
      ],
      "expanded_definition": "G10L13/00 covers techniques for speaker recognition or speaker verification.\n\nThis includes methods for recognizing or verifying the identity of a speaker based on analysis of their speech characteristics, such as vocal tract features, pitch, etc. The category is specifically focused on recognizing speakers rather than recognizing speech content."
    }
  ],
  "expanded_definition": "G10L covers techniques and methods specifically focused on speech analysis, speech synthesis (text-to-speech), speech recognition (converting spoken words to text), speech/voice processing (e.g., enhancing speech signals, speaker identification), and coding/decoding of speech or audio signals."
}